---
created:
  - " 10-24-2024 14:01"
tags:
  - Classes
aliases:
---

# 📗 -> Lecture Date: Name
---
[Lecture Slide Link]

## 🎤 Vocab


## ❗ Unit and Larger Context
Small summary

## ✒️ -> Scratch Notes
### Optimization
Gradient descent
- Limit to a convex function
- Compute gradient at the current location
- Take a step down the gradient
- Repeat
- Local Optima = Global Optima = Derivative == 0
Step size (alpha) is very important
- If step too big, 


## 🧪-> Example
- List examples of where entry contents can fit in a larger context

## 🔗 -> Links
### Resources
- Put useful links here

### Connections
- Link all related words
