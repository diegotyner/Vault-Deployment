---
created:
  - " 10-24-2024 14:01"
tags:
  - Classes
aliases:
---

# ðŸ“— -> Lecture Date: Name
---
[Lecture Slide Link]

## ðŸŽ¤ Vocab


## â— Unit and Larger Context
Small summary

## âœ’ï¸ -> Scratch Notes
### Optimization
Gradient descent
- Limit to a convex function
- Compute gradient at the current location
- Take a step down the gradient
- Repeat
- Local Optima = Global Optima = Derivative == 0
Step size (alpha) is very important
- If step too big, 


## ðŸ§ª-> Example
- List examples of where entry contents can fit in a larger context

## ðŸ”— -> Links
### Resources
- Put useful links here

### Connections
- Link all related words
