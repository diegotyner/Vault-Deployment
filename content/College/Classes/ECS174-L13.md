---
created:
  - " 11-07-2024 13:58"
tags:
  - Classes
aliases:
---

# ðŸ“— -> Lecture Date: Name
---
[Lecture Slide Link](https://web.cs.ucdavis.edu/~hpirsiav/courses/CVf24/slides/11_cnn.pdf)

## ðŸŽ¤ Vocab
**Batch Normalization (BN)** 
**Residual Net (ResNets)**

## â— Unit and Larger Context
Small summary



## âœ’ï¸ -> Scratch Notes
### Batch Normalization
![[Batch-Normalization-Equation.png]]
Two modes of BN:
- Train mode
	- $\mu, \sigma$ are functions of x; backprop gradients
- Test mode
	- $\mu, \sigma$ are pre-computed on training set

### Deep Residual Learning
Deep ResNets can be trained without difficulties
They don't show the lower training error and test error that normal deep nets do


### Packages:
2 main deep learning packages
- TensorFlow
- PyTorch

### Breaking CNNs
Humans are susceptible to visual illusions, and CNNs are no different
We might not be able to detect this, as what they can detect is different. Noise is particularly effective.
![[Breaking-CNNs.png]]
- [Paper](http://arxiv.org/abs/1312.6199)
- Github: [Breaking ConvNets](http://karpathy.github.io/2015/03/30/breaking-convnets/)
Do *gradient ascent*, to *maximize* loss instead of minimize
#### Defense
Train the net on noisy images, this is called *adversarial training*


## ðŸ”— -> Links
### Resources
- Put useful links here


### Connections
- [[_ML-Study]]
- [[Cyber-Security]]
