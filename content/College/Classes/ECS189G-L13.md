---
created:
  - " 04-28-2025 18:21"
tags:
  - Classes/Lecture
aliases:
---

# 📗 ->  04/28/25: ECS189G-L13
---
[Lecture Slide Link](https://drive.google.com/file/d/1b9iyHLtr8SnYS-qWz1x5UtNOZv6MLCrg/view)

## 🎤 Vocab



## ❗ Unit and Larger Context
Small summary




## ✒️ -> Scratch Notes
### Discriminative models vs Generative models
#### Discriminative models
Based on seen data (training data)
Learn a mapping $f_{model}$ to fit the training data set
- Assumption: distribution of training data is similar to a underlying distribution that governs all data, including unseen ones
A MNIST classifier is an example of a discriminative model

#### Generative Models
We create a training dataset out of all data, and build a model that can *generate* a data instance sample. No ground truth.

*AIGC* - AI Generated Content
*SAM* - Segment Anything Model (META AI)
- segment-anything.com | includes an online demo
*Genesis* - Combine generative models with physics engine
- genesis-embodied-ai.github.io

Many problems require generative models:
- Speech synthesis: text -> speech audio
- Machine translation: EN -> FR, CN -> JP
- Image segmentation: input image -> segmented regions

#### Auto-encoders
The second half (the decoding) of an auto-encoder is just like a generative model
##### Problems
- Doesn't try to simulate real data, only copy inputs
	- If there are no inputs, cannot generate embeddings
- Cannot generalize well to unseen images
	- Minor change to inputs can generate very different outputs
- Limited application in diverse learning settings
	- AE are unsupervised, difficult to generalize

#### Games
Game theory is the stud of mathematical models for strategic interactions among rational agents
- Assumes: all agents are selfish and aim to maximize payouts


*Nash Equilibrium* - Each agent's strategy is the 'best' possible for their opponent's strategy

Prisoner's Dilemma:
- For each of them, it is payoff maximizing in 1 match to betray. 
Zero-sum game
- No wealth is created or destroyed, any person's gain is another's loss
Min-Max game

### GAN - Generative Adversarial Network
A game between discriminative model and generative model
- *Discriminator D:* Distinguish data instances samples from data distribution from those generated by the generator G
- *Generator G:* Tries to trick the discriminator D by generating data instances that are as hard as possible to distinguish
	- Hard to distinguish $\approx$ Real data
- A *min-max* between D and G
After running the model, you will get loss functions D and G

#### Objective Function:
$min_Gmax_DV(D,G) = E_{x_\sim p_{data}(x)}[\;logD(x)\;]+E_{z_\sim p_z(x)}[\;log(1-D(G(z)))\;]$
- $D(\vec{x})$: Probability of x to be really D
- $G(\vec{z})$: Generation output by G on noise $\vec{z}$
	- Leads to $D(G(\vec{z}))$
- The expectation: $E(f(x)) = \int f(x)\cdot P(x)dx$


## 🧪 -> Refresh the Info
> Did you generally find the overall content understandable or compelling or relevant or not, and why, or which aspects of the reading were most novel or challenging for you and which aspects were most familiar or straightforward?)  
```

```

> Did a specific aspect of the reading raise questions for you or relate to other ideas and findings you’ve encountered, or are there other related issues you wish had been covered?)
```

```




## 🔗 -> Links
### Resources
- Put useful links here


### Connections
- Link all related words
