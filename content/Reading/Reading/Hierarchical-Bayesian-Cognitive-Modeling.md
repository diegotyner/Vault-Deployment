---
tags:
  - Entry
Entry-For: _Research-Papers
topic: Hierarchical Bayesian Cognitive Modeling
created:
  -  12-15-2024 21:21
---

# Hierarchical Bayesian Cognitive Modeling


Link to Source: [link](https://www.sciencedirect.com/science/article/pii/S0022249610001148)
- February 2011
---

### Quick Summary:
- Methodology: What was their experimental design? How did they induce their results?
- Findings: What did they expose with their research? What was surprising?
### Takeaway:
- What is the relevance to our use case?
- What do they do that we should emulate?

---

### Scratch Notes:
#### Abstract
Most important benefits of Hierarchical Bayesian modeling:
1) Development of more complete theories, can accounts for individual differences in cognition
2) Can account for observed behavior in terms of multiple cognitive processes
3) Involves using a few key psych variables to explain behavior on a wide range of cognitive tasks
4) Conceptual unification and integration of disparate cognitive models

#### 1. Introduction
> Bayesian statistics provides a compelling and influential framework for representing and processing information.

Multiple ways to conduct Bayesian Analyses
Theoretically
- *Objective Bayesian* - [Jaynes (2003)](https://scholar.google.com/scholar_lookup?title=Probability%20theory%3A%20the%20logic%20of%20science&publication_year=2003&author=E.T.%20Jaynes) 
- *Subjective Bayesian* - de Fineti (1974)

Bayesian statistics represents knowledge and uncertainty about variables in probability distributions. These representations are updated/manipulated using probability theory.
This is distinct from how frequentist / sampling distributions handle uncertainty? (how?)

Bayesian methods can be used statistically, for analyzing data.
Can also bet used theoretically, to guide interpretation of the mind and its inferences
- Help at explaining the brain computationally, ignoring mechanisms
Can also be used to relate models of psychological processes to data.
- Not intending to create statistical models like a generalized linear model (*GLM*), but trying to fit some aspect of cognition to behavioral or other data. 
- Statistical models attempt to infer dependent variables (recall, reaction times), modeling applications infer latent variables (capacities, learning rate)
The authors argue that this approach of using Bayesian methods to fit psychological processes to data is valuable, because they can deal with models of cognition without assumptions. 


#### 2. Benefits of using hierarchical Bayes in cognitive modeling
Defining **hierarchical Bayes** vs **non-hierarchical Bayes**:
##### Non-Hierarchical modeling
Any model that is more complicated than the simplest possible type of model, where a set of parameters $\theta$ generate a set of data $d$ through a likelihood function $f(\cdot)$. 
- $f(\theta) \to d$
This is a very limited definition of a non hierarchical model, that encompasses a broad majority of successful models of cognition.
- Accommodates Signal Detection Theory. Measures discriminability and a response criterion or bias.
	- hits, false alarms, s and n noise trials. 
	- $d = (h,f,s,n)$. Then the likelihood function that formalizes SDT, $d \sim f(\theta)$
- Generalized Context Model of category learning.
- Multidimensional scaling models provide amapping betwen latent coordinate locations of stimuli and their observed juged pairwise similarities
- Ratcliff diffusion model maps from parameters of bias, caution, evidence, and more to accuracy and response times for simple decisions.

##### Developing deeper theories
Hierarchical models are more complicated, and one example is:
- Parameters $\theta$ are generated by some other process $g(\cdot)$, parameterized by $\psi$, sometimes called hyper-parameters.
- In a hierarchical model, it is also important to say how these basic parameters $\theta$ are generated. Instead of just giving a mapping $f(\theta)\to d$, we need $\theta \sim g(\psi);\; f(g(\psi)) \to d$. 

The best example of problem for this structure is accomodating individual differences. Clear in cognition and practice, but difficult with non-hierarchical models (im just going to call them **NHM**s and **HM**s from now on).

[Shiffrin et al. 2008](https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20model%20evaluation%20approaches%20with%20a%20tutorial%20on%20hierarchical%20Bayesian%20methods&publication_year=2008&author=R.M.%20Shiffrin&author=M.D.%20Lee&author=W.-J.%20Kim&author=E.-J.%20Wagenmakers) does this in practice for a memory retention task. $d$ is how often memory items are recalled over time, $f$ is a memory retention function like and exponential or power function, $\theta$ re starting points, decay rates, and other properties of retention functions. Hierarchically, $g$ might be a normal distr with a mean and variance in $\psi$ that describes the dist over staring points and decay rates across individuals. 

Other uses:
- Memory
- Decision-making
- Confidence
- Emotional states

[Kemp et al. 2007](https://scholar.google.com/scholar_lookup?title=Learning%20overhypotheses%20with%20hierarchical%20Bayesian%20models&publication_year=2007&author=C.%20Kemp&author=A.%20Perfors&author=J.B.%20Tenenbaum) use this HM to model "basic inductive processes in cognitive development that require learning what they term ‘overhypotheses’."
- Overhypotheses: mappings $g$ that constrain $\theta$. "constraints on the hypotheses considered by the learner". 


##### Linking psychological variables to multiple phenomena
A different sort of HM, where parameters $\theta$ generate many sort of data $d_1 \dots d_n$ through a range of likelihood functions $f_1 \dots f_n$. The same psychological variables influence behavior on multiple tasks. 
- Seeks the unification of cognitive science
- $f_1(\theta)\to d_1 \land f_2(\theta)\to d_2 \land \dots \land f_n(\theta)\to d_n$
- $f_1$ could be recognition, $f_2$ could be recall, and so on.
*Essentially saying that people have qualities $\theta$ that are used in different tasks through different f functions. *


##### Linking psychological phenomena to multiple processes
> a hierarchical model that allows for multiple cognitive processes to contribute to a single set of observed data

Processes $f_1 \dots f_n$ and associated parameters $\theta_1 \dots \theta_n$. They mix to form an observed data $d$ through a mixing process $h$ parameterized by $z$. 
Possibilities for $h$:
- Choosing one of the process $f$ through probabilities $z$
- $h$ mixes all of the process together according to proportions $z$ 
*Essentially a melting pot, a large number of processes and parameters blended through a process h*

Writing and word selection being a mixture process of different semantic topics. Mixture assumption important to explain semantic context and homonyms. Also showed a study that used it to explain development, showing knowledge of concepts could be described as a mixture of different sorts of behavior and underlying developmental stages.

##### Unifying different models
Take it one step further, and generate the parameters for the multiple process model
- $f1 \dots f_n \to d$, combination rule $h$ governed by $z$. 
- The parameters $\theta_1 \dots \theta_n$ are generated by some process $g$ governed by $\psi$

> Perhaps most fundamentally, Vanpaemel (2011) argues in this special issue that linking models hierarchically is one way to address the basic Bayesian need to specify theoretically meaningful priors. The key idea is that the prior predictive distribution of the hierarchical part of the model, which indexes different basic models, naturally constitutes a psychologically interpretable prior over those models. This is a powerful idea, running counter to a current prejudice for making priors as uninformative as possible, and deserves to be an active area of research in using hierarchical Bayesian methods to model cognition


#### Conclusion
NHM models are very popular for modeling, create a task, find some parameters that can fit model to data and behavior, call it a day. But this is limited in its explanatin of humanity.

HM methods can broaden scope of cognitive models. 
1) Model development can be done at multiple levels of abstraction
2) Allow the use of same psych variables to account for behavior over multiple tasks
3) Allow data to be understood quantitatively and qualitatively as a mixture
4) Can unify disparate models, ground specification of priors as well
> In short, hierarchical Bayesian approaches demand our accounts of cognition become deeper and better integrated. The aim of this special issue is to provide some concrete examples of the potential of hierarchical Bayes in practice, for models ranging from memory, to category learning, to decision-making. We hope that they are useful early exemplars of what should become an important and widespread way of building and analyzing models of cognition.



## ❓-> Questions during reading
> The Bayesian statistical approach is distinct from how frequentist / sampling distributions handle uncertainty? (how?)

> What the hell does this have to do with Bayes Theorem?


## 🧪 -> Refresh the Info
> Did you generally find the overall content understandable or compelling or relevant or not, and why, or which aspects of the reading were most novel or challenging for you and which aspects were most familiar or straightforward?)  
```

```

> Did a specific aspect of the reading raise questions for you or relate to other ideas and findings you’ve encountered, or are there other related issues you wish had been covered?)
```

```
